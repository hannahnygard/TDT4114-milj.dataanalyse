{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca2ad17f",
   "metadata": {},
   "source": [
    "# **Del 1**\n",
    "### **Oppgave 2**\n",
    "\n",
    "1.\t**Hvilke åpne datakilder er identifisert som relevante for miljødata, og hva er kriteriene (f.eks. kildeautoritet, datakvalitet, tilgjengelighet, brukervennlighet osv.) for å vurdere deres pålitelighet og kvalitet?** <br>\n",
    "\n",
    "Vi brukte i hovedsak meterologisk instututt som kilde for innhenting av miljødata. Vi ser på dette som er relevant, men først og fremst pålitelig kilde. MET og Frost skriver på nettsiden, under \"What is Frost?\" at datakvaliteten kontrolleres daglig, månedlig og årlig. <br> <br>\n",
    "Likevel er det verdt å trekke frem at alle målingene vi har, ligger under \"preformanceCategory C\", som i følge Frost selv vil si at sensoren brukt for å ta målingene oppfyller kravene fra WMO og CIMO (World Meteorological Organization og Commission for Instruments and Methods of Observation), men mangler nødvendige målinger for kontroll, kalibrering og vedlikehold. Det vil si at selv om vi regner dataen som pålitelig og relativt feilfri, så er datarensen en viktig prosess for å likevel luke ut eventuelle feil.\n",
    "\n",
    "\n",
    "2.\t**Hvilke teknikker (f.eks. håndtering av CSV-filer, JSON-data) er valgt å bruke for å lese inn dataene, og hvordan påvirker disse valgene datakvaliteten og prosessen videre?** <br>\n",
    "\n",
    "Vi hentet data via API-kall og lagret dem enten i JSON-filer eller i databaser med pandasSQL. JSON, noe som viste seg å være det enkleste formatet. Vi forsøkte først å lagre data i CSV-filer, men støtte på problemer da filene, til tross for .csv-endelsen, ikke oppførte seg som standard CSV-filer. Dette hindret videre behandling, og vi valgte derfor å utelukkende bruke JSON. <br><br>\n",
    "JSON-formatets likhet med lister og ordbøker gjorde det enkelt å benytte teknikker vi allerede hadde lært i tidligere emner. Dette forenklet både rensing og kvalitetssikring av data, da vi kunne bruke kjente metoder. Vi vurderer at dette har hatt en positiv innvirkning på datakvaliteten, ettersom vår forståelse av verktøyene vi brukte bidro til et mer nøyaktig og pålitelig resultat.\n",
    "\n",
    "3.\t**Dersom det er brukt API-er, hvilke spesifikke API-er er valgt å bruke, og hva er de viktigste dataene som kan hentes fra disse kildene?** <br>\n",
    "\n",
    "Som nevnt er det meterologisk institutt sin API, frost, vi har benyttet oss av. Vi valgte å bruke kun denne API-en, da vi fant denne mer brukervennlig enn andre API-er vi fant på nett. Selv om den inneholdt få datafeil, har vi likevel opplevelsen av at vi har vist hvordan vi hadde håndtert det basert på det vi fant. En annen grunn til at vi landet på å bruke Frost, var at det var enormt mye data å hente her, med mange elementer fra mange år.\n",
    "\n",
    "\n",
    "### **Oppgave 3**\n",
    "\n",
    "1. **Hvilke metoder vil du bruke for å identifisere og håndtere manglende verdier i datasettet?** <br>\n",
    "\n",
    "Vi sjekker gjennom hver rad og sjekker at hver dato/årstall har en tilhørende verdi, dette for å utelukke manglende verdier. Originalt fant vi ingen manglende verdier, men har implementert kode for datamanipulering, slik at vi får testet om koden for datarens fungerer i praksis. I de manglende verdiene som vi har manipulert setter vi inn gjennomsnittet for alle årene fra 1980 til og med 2020. Vi synes gjennomsnitt virker som det beste alternativet fremfor andre metoder som median og typetall. <br><br>\n",
    "Ettersom det ikke er store variasjoner vil gjennomsnitt gi et bedre bilde på sentraltendensen i dataene sammenlignet med median, og typetall gir lite mening i datasett med kontinuerlige verdier og få eller ingen gjentagelser. Ved gjennomsnitt vil datastrukturen sin balanse i større grad bli beholdt, og det gir en mer nøytral erstatning som ikke forvrenger datasettets helhetlige statistikk. \n",
    "\n",
    "2. **Kan du gi et eksempel på hvordan du vil bruke list comprehensions for å manipulere dataene?** <br>\n",
    "\n",
    "Vi bruker noe som ligner list comprehensions i funksjonen «vis_statistikk». Her benyttes en generator expression for å kontrollere at alle elementer i listen «verdier» er tall (int eller float). Denne metoden ligner list comprehensions i syntaks, men i stedet for å lage en midlertidig liste, evaluerer den elementene enkeltvis på en effektiv måte. Dette er en smart måte å validere data uten å bruke ekstra minne til en mellomliggende liste. \n",
    "\n",
    "3.\t**Hvordan kan Pandas SQL (sqldf) forbedre datamanipuleringen sammenlignet med tradisjonelle Pandas-operasjoner?** <br>\n",
    "\n",
    "Pandas SQL kan forbedre datamanipuleringen ved at komplekse spørringer som joins og aggregeringer blir enklere og mer intuitiv å skrive og lese for de som er vant til SQL. Det gir samtidig raskere prototyping uten behov for en egen database, mens tradisjonelle pandas-operasjoner ofte er mer fleksible kan de bli mer komplekse og mindre lesbare for avanserte transformasjoner.\n",
    "\n",
    "4.\t**Hvilke spesifikke uregelmessigheter i dataene forventer du å møte, og hvordan planlegger du å håndtere dem?** <br>\n",
    "\n",
    "Vi forventer å møte flere uregelmessigheter i dataene. Blant annet manglende verdier som oppstår ved at enkelte datapunkter kan være satt til None, duplikater i form av at flere rader for samme år gjentas, samt inkonsistente datatyper der enkelte kolonner inneholder lister eller ordbøker i stedet for enkeltverdier. Det kan også forekomme utfordringer tilknyttet datoformatet, som vi i vårt tilfelle må konverteres fra tekst til datetime og deretter til årstall for enklere gruppering. For å håndtere uregelmessigheter i dataene har vi laget ulike funksjoner som skal rense. Dette medfører at komplekse datatyper blir konvertert til strenger slik at de kan behandles i tabellformat, fjerning eller erstatning av manglende verdier med gjennomsnittet for hele datasettet, og at duplikatene fjernes slik at alle datapunktene er unike. Det er også viktig at kolonnenavnene er standardiserte, og at dataene er formatert på en måte som gjør den konsistent og klar for videre analyse.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75ca39",
   "metadata": {},
   "source": [
    "# **Del 2**\n",
    "### **Oppgave 4**\n",
    "1.\t**Hvordan kan du bruke NumPy og Pandas til å beregne gjennomsnitt, median og standardavvik for de innsamlede dataene, og hvorfor er disse statistiske målene viktige?** <br>\n",
    "\n",
    "Bibliotekene Numpy og Pandas er til stor hjelp når man skal bearbeide og analysere store mengder data. Med disse verktøyene kan man på en rask og effektiv måte beregne statistiske mål slik som gjennomsnitt, median og standardavvik. Slike mål er viktige fordi de gir oss innsikt i hvordan dataene er fordelt og hjelper oss å forstå skjevheter og variasjon i datasettene. Dette danner et godt grunnlag for videre analyse og prediksjoner. <br><br>\n",
    "Gjennomsnitt viser til den mest typiske verdien i datasettet. Dette beregnes ved å summere alle verdiene og deretter dele på antallet. I pandas gjøres dette enkelt med .mean() eller i numpy med np.mean(). <br><br>\n",
    "Median er den midterste verdien når dataene sorteres i stigende rekkefølge. Dette statistiske målet er spesielt nyttig når datasettet inneholder ekstreme verdier, da gjennomsnitt kan gi et misvisende bilde. Median kan beregnes med .median() i pandas eller np.median() i numpy. <br><br>\n",
    "Til slutt har vi standardavvik. Dette statistiske målet forteller oss hvor mye verdiene i datasettet varierer fra gjennomsnittet. Et høyt standardavvik indikerer stor variasjon, mens et lavt standardavvik betyr at verdiene ligger tett samlet rundt gjennomsnittet. Dette målet kan beregnes med .std() i pandas eller np.std() i numpy. \n",
    "\n",
    "2.\t**Kan du gi et eksempel på hvordan du vil implementere en enkel statistisk analyse for å undersøke sammenhengen mellom variabler i datasettet?** <br>\n",
    "\n",
    "For å undersøke sammenhengen mellom variabler i datasettet, kan man bruke en statistisk analyse, slik som korrelasjonsanalyse. Dette er en metode som gir innsikt i hvor sterkt og hvilken grad to variabler henger sammen. Ved hjelp av pandas kan man bruke .corr()- funksjonen og finne korrelasjonskoeffisienten mellom variablene. Resultatet vil vise verdier mellom -1 og 1 for hver av kombinasjonen av variabler. <br><br>\n",
    "En verdi nær 1 betyr at det er en sterk positiv sammenheng. F.eks at det blir høyere temperatur, når skydekke øker. En verdi nær -1 betyr at det er en sterk negativ sammenheng. Og til slutt vil en verdi nær 0 bety liten eller ingen sammenheng. <br><br>\n",
    "Ved å gjøre en korrelasjonsanalyse vil en se en klarere sammenheng mellom variablene. Man vil for eksempel se at ved mer skydekke vil det også bli høyere temperatur eller at nedbør har en sammenheng med hvor skyet det er. <br><br>\n",
    "Videre kan man også lage en korrelasjonsmatrise ved å bruke seaborn. Dette vil gi et mer visuelt bilde av sammenhengen mellom variablene. \n",
    "\n",
    "3.\t**Hvordan planlegger du å håndtere eventuelle skjevheter i dataene under analysen, og hvilke metoder vil du sikre at analysen er sikker på?** <br>\n",
    "\n",
    "For å håndtere eventuelle skjevheter i dataene under analysen er det viktig å først kunne identifisere dem, og deretter bruke passende metoder for å redusere påvirkningen på analysen. Skjevheter kan for eksempel oppstå som følge av duplikater, manglede verdier eller ekstremverdier. <br><br>\n",
    "Første steg blir derfor å rense dataene. Duplikater fjernes for å unngå at enkelte observasjoner får for stor vekt i analysen. Videre blir manglende verdier erstattet. Her valgte vi å erstatte manglende verdier med gjennomsnittet av de eksisterende verdiene for å bevare dataen sin struktur. <br><br>\n",
    "Dersom det er mange manglende verdier, bør man vurdere alternative metoder. For eksempel med median istedenfor gjennomsnitt, eller maskinlæringsbaserte teknikker slik som nevrale nettverk. Denne vil estimere manglende verdier basert på mønstre i resten av datasettet. <br><br>\n",
    "Til slutt bør man sikre at analysen er pålitelig. Dette kan gjøres gjennom validering av resultater (test/train-splitting) eller visualisere dataene for å fange opp uvanlige mønstre. \n",
    "\n",
    "4.\t**Hvilke visualiseringer vil du lage for å støtte analysen din, og hvordan vil disse visualiseringene hjelpe deg med å formidle funnene dine?** <br>\n",
    "\n",
    "For å støtte analysen vår har vi brukt flere ulike type visualiseringer, som hver for seg har et bestemt formål. <br>\n",
    "Vi har tatt i bruk linjediagrammer for å vise hvordan variablene; skydekke, temperatur og nedbør, endrer seg over tid. Disse diagrammene hjelper oss å identifisere trender, mønstre og eventuelle endringer i datasettene. <br>\n",
    "Vi har også gruppert dataene etter tiår og visualisert de gjennomsnittlige verdiene med stolpediagram. Dette gir en tydelig oversikt over langtidstendenser og gjør det enklere å sammenligne perioder. Slike diagrammer er nyttige for å vise endringer i klimaet over tid i et enkelt format. <br><br>\n",
    "For den prediktive analysen har vi visualisert fremtidige verdier ved hjelp av polynomregresjon. Disse kurvene vises sammen med de faktiske dataene. Hvilken funksjon som velges, bestemmes ut ifra hvilken modell som gir høyest r2-score. Denne scoren vil indikere hvor godt modellen passer til dataene. Disse grafene gir en fremstilling av mulige fremtidige utviklinger basert på historiske trender. \n",
    "\n",
    "\n",
    "### **Oppgave 5**\n",
    "1.\t**Hvilke spesifikke typer visualiseringer planlegger du å lage for å representere eksempelvis endringer i luftkvalitet og temperaturdata, og hvorfor valgte du disse?** <br>\n",
    "\n",
    "Vi valgte følgende visualsieringer for vår data:\n",
    "•\tLinjediagram:\n",
    "For å illustrere hver variabel i sin helhet, fra 1980-2020, valgte vi å bruke linjediagram. Dette gjør det enklere å se trender og utviklinger, samt å identifisere ekstreme avvik i dataen. Ved å inkludere markører for statistiske mål (median, gjennomsnitt og standardavvik) ble dataen tydeligere og mer egnet for fortolkning . <br>\n",
    "* Søylediagram: <br>\n",
    "For å illustrere de statistiske målene gruppert på intervaller, benyttet vi oss av søylediagrammer for gjennomsnitt, samt markører for median og standardavvik. Hensikten med dette er muligheten for å sammenligne perioder, noe som er nyttig i en klimaanalyse over tid. <br>\n",
    "* Korrelasjonsmatrise: <br>\n",
    "Vi benytter oss av en korrelasjonsmatrise for å vise graden av sammenheng/korrelasjon som eksiterer mellom de tre variablene vi ser på. Dette hjepler oss å danne et bilde på i hvilken grad elementene faktisk påvirker hverandre. <br>\n",
    "* Tabellvisning (display(dataFrame)): <br>\n",
    "Til slutt er det verdt å nevne muligheten for å printe ut selve dataframen, per element, der all dataen vår ligger. Dette gir et oversiktlig og nøyaktiv fremstilling av dataen vi har hentet. <br>\n",
    "\n",
    "2.\t**Hvordan kan Matplotlib og Seaborn brukes til å forbedre forståelsen av de analyserte dataene, og hvilke funksjoner i disse bibliotekene vil være mest nyttige?**<br>\n",
    "\n",
    "De er begge nyttige og kraftige verktøy for forståelsen i en dataanalyse. De gjør det mulig å visualisere mønstre og trender, noe som gjør det enklere for oss mennesker å fatte. Samtidig gjør de det enklere å avdekke sammenhenger mellom variabler, for eksempel ved å plotte flere data inn i samme plt.plot(). Viktigst av alt gjør de kommunikasjonen av data utad mye enklere, der formidlingen av data kan gjøres til nesten hvem som helst, og de likevel vil få en viss oppfatning av hva dataen betyr.\n",
    "Hvilke funksjoner som er mest nyttige vil variere ut ifra hva man ønsker å vise, men noen av de mest elementere er nok, etter vår oppfatning, disse: <br>\n",
    "* plt.show() <br>\n",
    "* plt.bar() / plt.plot() <br>\n",
    "* Funksjoner som plt.legend()/title()/xlabel()/ylabel() er ikke kritiske i en visualisering, men de er alle veldig nyttige for å gjør dataen mer informativ og intuitiv <br>\n",
    "* sns.heatmap()\n",
    "\n",
    "3.\t**Hvordan vil du håndtere og visualisere manglende data i grafene dine for å sikre at de fortsatt er informative?**<br>\n",
    "\n",
    "Vi valgte å implementere et linjediagram som både vider den urensede dataframen i en farge, sammen med den rensede dataframen i en annen. Dette gjør at brukere enkelt og oversiktlig ser hvilke verdier som i utgangspunktet mangler, og hva som er puttet inn som en erstatning. Viktigst av alt lar dette brukeren se hvilket forhold denne “erstatningsverdien” har til resten av dataen, som er viktig for vedkommens oppfatning og troverdigheten av analysen.\n",
    "\n",
    "4.\t**Kan du beskrive prosessen for å lage interaktive visualiseringer med Widgets, Plotly eller Bokeh, og hvilke fordeler dette kan gi i forhold til statiske visualiseringer?** <br>\n",
    "\n",
    "Vi valgte å implementere en dropdown-meny med en knapp som genererer visualisering av dataen, der vi lar brukeren selv velge hva som skal utfylle den manglende verdien. Her har vi altså en meny med tre valg, som har en defaultverdi som gjennomsnitt. Videre har vi koblet en knapp til funksjonen som kjører plottingen av grafen, dette kobles gjennom observe()/on-click(). Input-elementet er dermed valget fra dropdown-menyen, funksjonene gjøres basert på input (ved hjelp av if-setninger), og output blir selve plottingen av den relevante grafen. <br><br>\n",
    "Fordelene med interaktive visualiseringer vs. statiske er flere. Ved at brukeren selv kan endre variabler og filtre, kan det gjøre opplevelsen mer engasjerende. Dette kan igjen gi en lettere, og bedre, innsikt og dypere forståelse av dataen. I tillegg tillater disse bibliotekene oss å lage mer kreative og bredere visualiseringer, noe som kan bedre utforskningen av dataen og innsikten vi får. Eksempelvis kan brukeren studere spesifikke områder nærmere, eller vi kan implementere mer detaljer gjennom hoved-effekt, uten at det generelle plottet overlesses. Interaktive visninger kan også sikre en mer målrettet og personlig visning, der brukeren filtrerer og grupperer den dataen vedkommende selv ser på som viktig og relevant. Dermed blir dataen aktuell for en bredere målgruppe med en variasjon av brukere.\n",
    "\n",
    "5.\t**Hvordan vil du evaluere effektiviteten av visualiseringene dine i å formidle de viktigste funnene fra dataanalysen til et bredere publikum?** <br>\n",
    "\n",
    "De visualiseringene vi har implementert i vårt prosjekt, ser vi på som informative og oversiktlige for et bredt publikum. Vi ser på dem som tydelige og forståelige - også for brukere med mindre teknisk bakgrunn. Ved å implementere beskrivende titler og aksetitler mener vi det er med på å heve brukervennligheten. Vi syntes hvert diagram bidrar med noe unikt, og at ingen av infoen er overlappene. For eksempel bidrar søylediagrammet bidrar med mulighet for sammenligning er over tid, linjediagrammet gir et generelt overblikk og utviklingen, mens korrelasjonsmatrisen illustrerer sammenhenger. Med fokus på brukervennlighet og universell utforming har vi gjort bevisste valg hva gjelder farger i diagrammene våre, der vi har hatt fokus på kontraster og synlighet, samtidig estetikk, noe vi håper vi har evnet.\n",
    "\n",
    "\n",
    "### **Oppgave 6**\n",
    "1.\t**Lag minst tre forskjellige typer visualiseringer (f.eks. linjediagrammer, søylediagrammer og scatterplots) for å representere endringer i eksempelvis luftkvalitet og temperaturdata over tid. Forklar valget av visualiseringstype for hver graf.** <br>\n",
    "\n",
    "Vi har valgt å visualisere vår data i form av linjediagram, stolpediagram og korrelasjonsmatrise, for å best mulig fremstille endringer og sammenhenger i værdata over tid. Værdataen som vi har hentet er skydekke, temperatur og nedbør. <br> <br>\n",
    "Først visualiserte vi hver enkelt variabel i form av stolpe og linjediagram for å vise utviklingen over tid og fordeling per tidsenhet. I denne sammenhengen viste linjediagrammet et godt overblikk over trendene for hver enkelt værvariabel, mens stolpediagrammene egnet seg godt til å sammenligne gjennomsnittsverdier og variasjon per tiår. <br><br>\n",
    "For å gi en mer samlet oversikt mellom de ulke variablene, lagde vi en kombinert visualisering som et felles linjediagram, der alle tre typer værdata var synlig, samt en korrelasjonsmatrise som illustrerer samsvarsgraden mellom skydekke, temperatur og nedbør. Denne korrelasjonsmatrisen gir en tydelig fremstilling av hvilke værfaktorer som statistisk henger sammen, noe som er viktig for å kunne se mønster eller potensielle årsakssammenhenger. <br><br>\n",
    "Linjediagram ble i vår besvarelse benyttet fordi den egnet seg godt til å vise utvikling over tid, og fremheve trender og sesongvariasjoner. Stolpediagrammet ble benyttet grunnet dens gode evne til å fremheve forskjeller mellom perioder, noe som i vårt tilfelle gjorde det enklere å se variasjoner mellom grupper av år. Til slutt ble korrelasjonsmatrisen valgt for å kunne avdekke og visualisere sammenhenger mellom de tre værvariablene, slik at vi lett kunne se hvilke som varierte i takt. Så alle våre valg av visualiseringstype er dermed begrunnet i ønsket om å vise både utvikling, variasjon og sammenheng. \n",
    "\n",
    "2.\t**Implementer visualiseringer ved hjelp av Matplotlib og Seaborn. Inkluder tilpassede akser, titler, og fargepaletter for å forbedre lesbarheten og estetikk.** <br>\n",
    "\n",
    "I oppgaven har vi brukt både Matplotlib og Seaborn til å lage informative og estetisk tiltalende diagrammer i visualiseringsdelen. Ser vi på linjediagrammet i funksjonen «vis_statistikk» brukes Matplotlib til å vise årlige verdier, gitt tilpassende akser, titler og tydelige etiketter. Her er det også inkludert horisontale linjer som viser gjennomsnitt, median, samt et skygget område der standardavviksbredde er representert, noe som gjør det lettere å tolke visuelt. <br><br>\n",
    "For å vise endring per tiår har vi brukt Seaborn til å lage et stolpediagram. Det er lagt inn feilstolper som standardavvik, og en medianverdi både som en gjennomgående linje og som røde kryss hvert tiår.  <br><br>\n",
    "Bruken av både Matplotlib og Seaborn gir en helhetlig og lesbar fremstilling av datasettet, der farger, størrelser og forklarende elementer er med på å forbedre både lesbarhet og forståelsen av vår data. \n",
    "\n",
    "3.\t**Demonstrer hvordan manglende data håndteres i visualiseringene. Lag en graf som viser hvordan manglende verdier påvirker datatrender, og diskuter hvordan dette kan påvirke tolkningen av dataene.** <br>\n",
    "\n",
    "I vår oppgave har vi håndtert manglende verdier ved å erstatte dem med gjennomsnittet for perioden 1980 til 2020. For å illustrere hvordan ulike erstatningsmetoder kan påvirke datatrender, har vi utviklet widgets som lar brukeren visualisere hvordan dataserien ville sett ut dersom de manglende verdiene var erstattet med enten median eller interpolasjon. Dette gir innsikt i hvordan ulike metoder kan påvirke tolkningen av datamaterialet. Manglende data kan føre til feilaktige slutninger, for eksempel kan en variabel fremstå som stabil, mens det i realiteten mangler målinger fra perioder med høy variasjon. Derfor kan det være metodisk forsvarlig å erstatte slike verdier med representative statistiske mål, for å gi et mer helhetlig og troverdig bilde av utviklingen.\n",
    "\n",
    "4.\t**Skriv en kort evaluering av de utviklede visualiseringene. Diskuter hvilke visualiseringer som var mest effektive for å formidle informasjon, og hvorfor. Reflekter over tilbakemeldinger fra medstudenter eller veileder.** <br>\n",
    "\n",
    "De tre ulike visualiseringene linjediagram, stolpediagram og korrelasjonsmatrisen, hadde alle ulike styrker, men linjediagrammet fremsto som det best effektive verktøyet i formidlingen av værdata over tid. Dette diagrammet gjorde det enkelt å identifisere trender i temperatur, skydekke og nedbør. Det var også spesielt nyttig for å vise sesongvariasjoner og langsiktige endringer. <br><br>\n",
    "Stolpediagrammet virket på den andre siden bedre i from av å sammenligne gjennomsnittsverdier per tiår, og ga en god visuell oversikt over forskjeller mellom perioder, men var mindre egnet til å vise kontinuerlig utvikling. Korrelasjonsmatrisen var verdifull og ga oss innsikt i statistiske sammenhenger mellom variablene, men krever mer forkunnskaper for å kunne tolke, og er derfor mindre tilgjengelig i en bredere sammenheng. <br><br>\n",
    "Når målet vårt var å kommunisere utvikling over tid, på en lettfattelig måte, kan vi da si at linjediagrammet er best drøftet opp mot de andre, men at korrelasjonsmatrise kanskje er nyttigst når det er snakk om analytisk kontekst. I tillegg er linjediagrammet mer allsidig, ettersom det egner seg både til individuell visualisering av hver værvariabel og til en samlet framstilling av dem. <br><br>\n",
    "Tilbakemeldingene vi mottok fra veileder var svært nyttige og ga oss konkrete innspill til forbedringer både i struktur og dokumentasjon. Vi har blitt mer bevisste på viktigheten av å samle all kode i en strukturert mappe, som src, og å skille datafiler tydelig i data-mappen. Spesielt verdsatte vi forslaget om å samle funksjoner i en hovedfil, noe som gjør det enklere å kjøre prosjektet uten avhengigheter til flere enkeltfiler. Vi ser også verdien av bedre forklaring i Jupyter-notatbøkene og tydeligere dokumentasjon i README-filen, både for vår egen forståelse og for andres bruk og evaluering. Tilbakemeldingene har gjort oss mer bevisste på ryddighet, lesbarhet og brukervennlighet i kodebasen.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
